---
title: "Homework 3"
author: "Tongtong Zhu"
date: "2022-10-13"
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem 1

### Load and read `instacart` data

```{r}
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

**Description of data**

The `instacart` is a online grocery shopping dataset in 2017. It contains `r nrow(instacart)` observations and `r ncol(instacart)` variables. The key variables include `r colnames(instacart)`.


### Comment on the results

#### Aisle count and identify 

```{r}
instacart %>% 
  group_by(aisle_id, aisle) %>% 
  summarize(n_obs = n()) %>% 
  arrange(desc(n_obs))
  
```

There are `r max(pull(instacart, aisle_id))` aisles and `fresh vegetables`, `fresh fruits` and `packaged vegetables fruits` are the first, second and third aisles with the most items ordered from, respectively. 

#### Plot aisles

```{r}
instacart %>% 
  group_by(aisle) %>% 
  summarize(n_obs = n()) %>% 
  filter(n_obs > 10000) %>% 
  mutate(aisle = forcats::fct_reorder(aisle, n_obs)) %>% 
  ggplot(aes(x = n_obs, y = aisle)) +
  geom_col() +
  labs(
    title = "Number of items ordered per aisle",
    x = "Number of items",
    y = "Aisle name",
    caption = "Note: Limited to aisles with more than 10000 items ordered"
  )

```

#### Table three aisles with three most popular items

```{r}
instacart %>% 
  filter(aisle == "baking ingredients" |
         aisle == "dog food care" |
         aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(n_pop = n()) %>% 
  mutate(rank = min_rank(desc(n_pop))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n_pop)) %>% 
  knitr::kable()
  
```

In `baking ingredients` aisle, the three most popular items are `Light Brown Sugar`, `Pure Baking Soda` and `Cane Sugar`. In `dog food care` aisle, the three most popular items are ` Snack Sticks Chicketn & Rice Recipe Dog Treats` and `Small Dog Biscuits`. In `packaged vegetables fruits` aisle, the three most popular items are `Organic Baby Spinach`, `Organic Raspberries` and `Organic Blueberries`.

#### Table mean hour 

```{r}
instacart %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean
  ) %>% 
  rename(
    "Sun" = "0",
    "Mon" = "1",
    "Tue" = "2",
    "Wen" = "3",
    "Thur" = "4",
    "Fri" = "5",
    "Sat" = "6" ) %>% 
  knitr::kable(digits = 2)

```

In general, `Pink Lady Apples` is ordered earlier than `Coffee Ice Cream`.


## Problem 2

### Load, tidy and wrangle data

```{r}
accel_df = 
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "count"
  ) %>% 
  mutate(
    weekday_vs_weekend = ifelse(day %in% c("Saturday","Sunday"),"weekend","weekday"),
    minute = as.numeric(minute),
    day = factor(day),
    day = forcats::fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
  ) %>% 
  select(week, day, everything())
                            
```

**Description**

The tidied `accel_df` dataset contains `r nrow(accel_df)` observations and `r ncol(accel_df)` variables. The key variables include `r colnames(accel_df)`.


### Calculate the total activity per day and table the totals

```{r}
total_act = 
  accel_df %>% 
  group_by(week, day) %>% 
  summarize(total_activity = sum(count)) 
   
knitr::kable(total_act)
```

There is no apparent trends of total activity based on the table. And there is no regular difference in total activity between weekdays and weekends. 

### Plot 24-hour activity

```{r}
accel_df %>% 
  ggplot(aes(x = minute, y = count, color = day)) +
  geom_line(alpha = 0.5) +
  geom_smooth(se = FALSE) +
  labs(
    title = "24-hour activity time course per day",
    x = "Time",
    y = "Activity count") +
  scale_x_continuous(
    breaks = c(0, 180, 360, 540, 720, 900, 1080, 1260, 1440),
    labels = c("0:00", "3:00", "6:00", "9:00","12:00","15:00","18:00","21:00","24:00")
    ) +
  scale_y_continuous(
    breaks = c(0, 1500, 3000, 4500, 6000, 7500, 9000)
  ) +
  theme(legend.position = "right")
 
```

**Description and conclusion**

Based on this graph, 19:00-22:00 seems to be the time period with relatively high activity for most days. The activity count is highest around 21:00 on Friday and 12:00 on Sunday. The highest activity count is about 9000 for all days. 


## Problem 3

### Load `NY NOAA` data

```{r}
data("ny_noaa")
```

### Count missing data

```{r}
noaa_missing = ny_noaa %>% 
  summarize(
    prcp_missing = mean(is.na(prcp)),
    snow_missing = mean(is.na(snow)),
    snwd_missing = mean(is.na(snwd)),
    tmax_missing = mean(is.na(tmax)),
    tmin_missing = mean(is.na(tmin)))

knitr::kable(noaa_missing)
```

**Description**

The `ny_noaa` dataset provides weather data of New York State weather stations from Jan. 1, 1981 to Dec. 31, 2010. It contains `r nrow(ny_noaa)` observations and `r ncol(ny_noaa)` variables. The key variables include `r colnames(ny_noaa)`. There are `r count(distinct(ny_noaa, id))` distinct weather stations in New York State. This dataset contains extensive missing data, in which `tmax` and `tmin` have a higher percent of missing data, almost to 44% for each,  and `snow` and `snwd` have a lower percent of missing data.

### Clean data, create variables and wrangle data

```{r}
noaa_tidy = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month","day"), sep = "-") %>% 
  mutate(
    prcp = prcp / 10,
    tmax = as.numeric(tmax) / 10,
    tmin = as.numeric(tmin) / 10,
    year = as.numeric(year),
    month = month.name[as.numeric(month)],
    day = as.numeric(day)
    ) 
  
```

```{r}
noaa_tidy %>% 
  count(snow) %>% 
  arrange(desc(n))
```

**Description**

The variables of `prcp`, `tmax` and `tmin` are divided by 10 to get reasonable units, because their original units are tenths of standard units. The most commonly observed values for snowfall is 0.

### Plot average tmax in January and July

```{r}
noaa_tmax = noaa_tidy %>% 
  filter(month %in% c("January", "July")) %>% 
  group_by(id, year, month) %>% 
  summarize(tmax_mean = mean(tmax, na.rm = TRUE))

ggplot(noaa_tmax, aes(x = year, y = tmax_mean, color = month)) +
  geom_point(alpha = 0.3) +
  geom_smooth(alpha = 0.5, se = FALSE) +
  labs(
    title = "Average max temperature in January and July per station across years",
    x = "Year",
    y = "Average max temperature (℃) ",
    caption = "Data from the NOAA" ) +
  scale_x_continuous(
    breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)) +
  facet_grid(. ~ month) +
  theme(legend.position = "none") +
  scale_color_manual(values = c("blue", "red"))
      
```

**Description**

The average max temperature in January is within -10-10°C and it fluctuates a bit along years, but there is no apparent trends of the fluctuation. The average max temperature in July is within 23-30°C and it doesn't fluctuate significantly along years. Comparing the average max temperature of these two months, January has a lower average max temperature than that of July, and January fluctuates more significantly than July. Both months have some outliers, the most outlier for January is about -14°C and the most outlier for July is about 14°C. 



